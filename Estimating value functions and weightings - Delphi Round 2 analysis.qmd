---
title: "Estimating value functions and weightings - Delphi Round 2 analysis"
author: "Ewan McHenry"

format:
  html:
    embed-resources: true
    theme: cosmo
    code-fold: true       # Enables folding
    code-summary: "Show code"  # Optional: custom label on the fold button
    toc: true              # Optional: adds a table of contents
    toc-depth: 2
    toc-float: true       # Optional: makes the table of contents float on the page
---
```{=html}
<script>
  function toggleVisibility(id) {
    const el = document.getElementById(id);
    if (el.style.display === "none") {
      el.style.display = "block";
    } else {
      el.style.display = "none";
    }
  }
</script>
```

This page presents the analysis of the 2nd round of the Delphi process to quantitatively define woodland ecological condition: to estimate the value functions and weightings to be associated with each woodland condition indicator measurement when calculating an overall condition score.

The code used in the analysis is embedded within the report

```{r libraries, warning=F, message=FALSE}
library(tidyverse)
library(ggpubr)
library(grid)
library(gridExtra)
library(cowplot)
library(mgcv)
library(kableExtra)
library(Polychrome)
library(DT)
library(qgam)


source("Scripts\\functions_delphi_analysis.R") 

```

```{r config}
this.round <- 2

DOMIN_classes <- c("0", "1: <4% few", "2: <4% several", "3: <4% many", "4: 4-10%",
                   "5: 10-25%", "6: 25-33%", "7: 33-50%", "8: 50-75%", "9: 75-90%",
                   "10: 90-100%")

```

```{r load_data, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
load("Data\\Delphi round 2\\curated.RData")

r1_data <- prepare_round_data(1, "Data\\Delphi round 1\\", "response sheets\\", run_extraction = T)
r2_data <- prepare_round_data(2, "Data\\Delphi round 2\\", "response sheets\\", run_extraction = T)

all.respondents <- unique(c(r1_data$df$respondant_name, r2_data$df$respondant_name))
all.indicators <- unique(c(r1_data$df$indicator_name, r2_data$df$indicator_name))
```

# Survey completion and changes

The value functions and weights presented here are based on the `r sum(all.respondents %in% r2_data$completed_summary$respondent_names)` respondents that completed round 2 of the Delphi process.

```{r respondent_completion_table}
# table of respondents by round
respondent_table <- tibble(
  respondent_names = all.respondents,
  round1 = as.integer(all.respondents %in% r1_data$completed_summary$respondent_names),
  round2 = as.integer(all.respondents %in% r2_data$completed_summary$respondent_names)
)
respondent_table$both_rounds <- respondent_table$round1 * respondent_table$round2

respondent_table %>% arrange(desc(both_rounds)) %>% 
  mutate(
    round1 = ifelse(round1 == 1, "Yes", "No"),
    round2 = ifelse(round2 == 1, "Yes", "No"),
    both_rounds = ifelse(both_rounds == 1, "Yes", "No")
  ) %>% 
  select(respondent_names, round1, round2, both_rounds) %>% 
  kable() 

```

```{r combine_rounds}
combine_rounds <- function(r1, r2, round1 = 1, round2 = 2) {
  bind_rows(
    mutate(r1, round = round1),
    mutate(r2, round = round2)
  )
}

c.df <- combine_rounds(r1_data$df, r2_data$df) %>% 
  #remove rows with NA for value
  filter(!is.na(value)) %>%
  mutate(
    respondant_name = factor(respondant_name, levels = all.respondents),
    indicator_name = factor(indicator_name, levels = all.indicators)
  ) %>%
  arrange(respondant_name,indicator_num, round, measure)
c.completed_summary <- combine_rounds(r1_data$completed_summary, r2_data$completed_summary)
c.ind.matcher.df <- combine_rounds(r1_data$ind_matcher_df, r2_data$ind_matcher_df)
c.has_completed <- combine_rounds(r1_data$has_completed, r2_data$has_completed)
c.just.one.df <- combine_rounds(r1_data$just_one_df, r2_data$just_one_df)

all.sheets <- unique(c.df$sheet_name)


```

```{r reduced_df}
red.df <- c.df %>% 
  arrange(respondant_name,indicator_num, round, measure) %>% 
  # select only the columns we want to see
  select(respondant_name, sheet_name, measure, value, round)


```

# Curation and checking of responses

This section is pretty technical (if you want, skip to [Estimating value functions and generating lookup tables]). It documents the checks and changes done to the raw Delphi survey form data. Best skipped for the more general reader, but useful for those interested in the details of the data curation process.

::: {.toggle-section}
<button onclick="toggleVisibility('my-section')">Show/hide curation info</button>

<div id="my-section" style="display: none; margin-top: 1em;">


## Unchanged responses between rounds

I individually checked where value functions did not change between a respondent's rounds 1 and 2, to be sure it was not accidentally missed, and was satisfied that - where there was no change - this was intentional.

Table of respondent name - indicator pairs that are identical between rounds.

```{r t-unchanged_responses}
# find respondent name - indicator pairs that are identical between rounds
bk = 1
same_last = data.frame(respondent = NA, indicator = NA)

for (r in all.respondents) {
  for(i in all.sheets) {
      r1v <- c.df$value[c.df$respondant_name == r & c.df$sheet_name == i & c.df$round == 1]
      r2v <- c.df$value[c.df$respondant_name == r & c.df$sheet_name == i & c.df$round == 2]
      r1m <- c.df$measure[c.df$respondant_name == r & c.df$sheet_name == i & c.df$round == 1]
      r2m <- c.df$measure[c.df$respondant_name == r & c.df$sheet_name == i & c.df$round == 2]

        if ( # responses unchanged between round 1 and 2
          identical(r1v, r2v) & identical(r1m, r2m)) {
          same_last[bk,] = c(r, i)
          bk = bk + 1
    }
  }
}

datatable(
  same_last,
  extensions = 'Buttons',
  options = list(
    dom = 'Bfrtip',  # 'f' enables the search box
    buttons = c('copy', 'csv')
  )
)
```

## Individual indicator value function curation

```{r functions for ndividual indicator curation}
# Check that all respondents covered entire range of 0-100% native canopy cover, and that all value functions were within 0-100.
ind_checker <- function (data = c.df){ data %>%
    filter(sheet_name == this.sheet & round == 2) %>% 
    group_by(respondant_name) %>%
    summarise(
      min_value = min(value, na.rm = TRUE),
      max_value = max(value, na.rm = TRUE),
      min.measure = min(measure, na.rm = TRUE),
      max.measure = max(measure, na.rm = TRUE),
      .groups = "drop"
    ) %>%
    mutate(
      full_range_measure = (min.measure == min(min.measure) & max.measure == max(max.measure)),
      full_range_value = (min_value == 0 & max_value == 100)
    )
}

call_ind_mess <- function(data = ind_check){
  if(any(!ind_check$full_range_measure)) {
  for (i in which(!ind_check$full_range_measure)) {
    print(paste(ind_check$respondant_name[i], "does not cover full range of", min(ind_check$min.measure) , "-",
                max(ind_check$max.measure),  this.sheet))
  }}  else {
  print(paste("All respondents cover the full range of", min(ind_check$min.measure) , "-",
                max(ind_check$max.measure),  this.sheet))}
if(any(!ind_check$full_range_value)) {
  for (i in which(!ind_check$full_range_value)) {
    print(paste(ind_check$respondant_name[i], "has value function outside of 0-100"))
  }} else {
  print("All respondents have value functions within 0-100")
}}

```

For every indicator I checked that every respondent covered the full range of measures (e.g. 0-4 for herbivore damage, 0-100 for native canopy percentage) and that all valuescovered the range of 0-100. If not, I curated the data to furfill this, within code, based on perceived intent (which was deemed very clear in all instances).

### Age structure

Some respondents used different numbers of age categories.

Went through forms and altered, ensuring that intent (which seemed clear in all cases) was preserved. Now all respondents in both use 4 age classes.

```{r check_r2_age_structure}
this.sheet <- "Tree age distribution"

ind_check <- ind_checker()
call_ind_mess(ind_check)
```

Checked that all age categories were used by all respondents.

```{r different_agecats?}
# Looked like some folk used different number of tree age categories. Table of max number of tree age categories used by respondent and round. Found in round 1 DK, NRB and PA had 5,5 and 3 categories respectively. Went into forms and altered. Intent seemed clear in all cases.
# find max number of tree age categories used by respondent and round
nages <- c.df %>%
  filter(sheet_name == this.sheet) %>%
  group_by(round, respondant_name) %>%
  summarise(
    max_age_categories = max(measure, na.rm = TRUE),
    .groups = "drop"
  ) %>% 
  filter (max_age_categories != max(max_age_categories))

```

### Native canopy percentage

```{r curation of incomplete estiamtes}
this.sheet <- "Native canopy percentage "
# curation where incomplete (see below check_r2_native_canopy_percentage)
## "Dean K" min measure 10
### duplicate row w measure 10 and value 0, then change the measure to 0
dup.row <- which(c.df$sheet_name == this.sheet & 
                  c.df$round == 2 & 
                  c.df$value == 0 & 
                  c.df$measure == 10 & 
                  c.df$respondant_name == "Dean K") 
added.row <- c.df[dup.row, ] # duplicate row
added.row$measure <- 0 # change measure to 0
c.df <- rbind(c.df, added.row) # insert row to df

```

Checked that all respondents covered entire range of 0-100% native canopy cover, and that all value functions were within 0-100. They were not, they were curated to include, within code, based on percieved intent (which was deemed clear).

```{r check_r2_native_canopy_percentage}
ind_check <- ind_checker(c.df)

call_ind_mess(ind_check)
```

### Vertical structure

```{r vertical_structure_curation}
# curation where incomplete
this.sheet <- "Vertical structure"
# curation where incomplete (see below check_r2_vertical_structure)

# curation where incomplete (see below check_r2_native_canopy_percentage)
## A few folk didnt include 5 categories. Judging by vf shapes seems to have not thought of lowest strata - i.e.ground, so will assume same value for lowest 2 value estiamtes... this might have been an issue with the forms
### function to do that
add_combine_lowest_measures <- function(df, respondent, sheet_name, round = 2) {
  # Shift measures by +1 for this respondent and sheet only
  df$measure[df$sheet_name == sheet_name & 
                            df$round == round & 
                            df$respondant_name == respondent] <- 
      df$measure[df$sheet_name == sheet_name & 
                            df$round == round & 
                            df$respondant_name == respondent] + 1

  
  # Find row to duplicate
  dup.row <- which(df$sheet_name == sheet_name & 
                   df$round == round & 
                   df$value == 0 & 
                   df$measure == 2 & 
                   df$respondant_name == respondent)

  # Duplicate row, assign measure = 1
  if (length(dup.row) > 0) {
    added.row <- df[dup.row, ]
    added.row$measure <- 1
    df <- rbind(df, added.row)
  }

  return(df)
}

### Bob Epsom
c.df <- add_combine_lowest_measures(c.df, "Bob Epsom", this.sheet)
### David L
c.df <- add_combine_lowest_measures(c.df, "David L", this.sheet)
### David V
c.df <- add_combine_lowest_measures(c.df, "David V", this.sheet)
### Iain Mo
c.df <- add_combine_lowest_measures(c.df, "Iain Mo", this.sheet)
### Kylie Jo-Ma
c.df <- add_combine_lowest_measures(c.df, "Kylie Jo-Ma", this.sheet)
### Martin Hu
c.df <- add_combine_lowest_measures(c.df, "Martin Hu", this.sheet)
### Mick Br
c.df <- add_combine_lowest_measures(c.df, "Mick Br", this.sheet)
### Peter Lo
c.df <- add_combine_lowest_measures(c.df, "Peter Lo", this.sheet)
### Saul H
c.df <- add_combine_lowest_measures(c.df, "Saul H", this.sheet)
### Sonia
c.df <- add_combine_lowest_measures(c.df, "Sonia", this.sheet)



# curation where wrong scale used
## "Henry D" and "Rachel Penn" increase all measures by 1 (went from 0-4, not 1-5)
c.df <- c.df %>% 
  mutate(measure = ifelse(sheet_name == this.sheet & 
                        round == 2 & 
                        respondant_name == "Henry D"  , 
                        measure + 1, measure))

c.df <- c.df %>%
  mutate(measure = ifelse(sheet_name == this.sheet & 
                        round == 2 & 
                        respondant_name == "Rachel Penn"  , 
                        measure + 1, measure))

```

```{r check_r2_vertical_structure}
ind_check <- ind_checker(c.df)
call_ind_mess(ind_check)
```

### Tree species richness

```{r tree_species_richness_curation}
this.sheet <- "N tree & shrub spp."

# Jim Sm-Wr stopped at 90, duplicate that row and add it in, editing measure to 100
dup.row <- which(c.df$sheet_name == this.sheet & 
                  c.df$round == 2 & 
                  c.df$respondant_name == "Jim Sm-Wr" & 
                  c.df$measure == 90)
added.row <- c.df[dup.row, ] # duplicate row
added.row$measure <- 100 # change measure to 100
c.df <- rbind(c.df, added.row) # insert row to df
# Mick Br started at 5, duplicate that row and add it in, editing measure to 0
dup.row <- which(c.df$sheet_name == this.sheet & 
                  c.df$round == 2 & 
                  c.df$respondant_name == "Mick Br" & 
                  c.df$measure == 5)
added.row <- c.df[dup.row, ] # duplicate row
added.row$measure <- 0 # change measure to 0
c.df <- rbind(c.df, added.row) # insert row to df
# Nick RB started at 10, duplicate that row and add it in, editing measure to 0
dup.row <- which(c.df$sheet_name == this.sheet & 
                  c.df$round == 2 & 
                  c.df$respondant_name == "Nick RB" & 
                  c.df$measure == 10)
added.row <- c.df[dup.row, ] # duplicate row
added.row$measure <- 0 # change measure to 0
c.df <- rbind(c.df, added.row) # insert row to df
```

```{r check_r2_tree_species_richness}
ind_check <- ind_checker(c.df)
call_ind_mess(ind_check)
```

### Invasives

```{r invasives_curation}
this.sheet <- "Invasive plants % cover"
```

```{r check_r2_invasive_cover}
ind_check <- ind_checker(c.df)
call_ind_mess(ind_check)
```

### Deadwood

```{r deadwood_curation}
this.sheet <- "Deadwood"
# curation where incomplete (see below check_r2_deadwood)
add_deadwood_measure_zero <- function(df, respondent, the_sheet) {
  # Get the lowest existing measure for the respondent
  min_measure <- df %>%
    filter(sheet_name == the_sheet,
           round == 2,
           respondant_name == respondent) %>%
    summarise(min_measure = min(measure, na.rm = TRUE)) %>%
    pull(min_measure)

  # Find the row to duplicate
  dup.row <- which(df$sheet_name == the_sheet & 
                   df$round == 2 & 
                   df$respondant_name == respondent & 
                   df$measure == min_measure)

  # If such a row exists, duplicate and change measure to 0
  if (length(dup.row) > 0) {
    added.row <- df[dup.row, ]
    added.row$measure <- 0
    df <- rbind(df, added.row)
  }

  return(df)
}

## Those  lowest measure is 1 or 2,  with value 0, duplicate that row and add it in, editing measure to 0

respondents <- c("Bob Epsom", "Dean K", "Kylie Jo-Ma")

for (name in respondents) {
  c.df <- add_deadwood_measure_zero(c.df, name, this.sheet)
}

#multiply all deadwood measures by 16/12
c.df <- c.df %>%
  mutate(measure = ifelse(sheet_name == this.sheet & round == 2, measure * (16/12), measure))

```

```{r check_r2_deadwood}
ind_check <- ind_checker(c.df)
call_ind_mess(ind_check)
```

### Veteran trees

```{r veteran_trees_curation}
this.sheet <- "Veteran trees"

# curation where incomplete (see below check_r2_veteran_trees)
## Dont have measrure 12, duplicate their max measure row and change the measure to 12
add_veteran_measure_twelve <- function(df, respondent, the_sheet) {
  # Get the highest existing measure for the respondent
  max_measure <- df %>%
    filter(sheet_name == the_sheet,
           round == 2,
           respondant_name == respondent) %>%
    summarise(max_measure = max(measure, na.rm = TRUE)) %>%
    pull(max_measure)

  # Find the row to duplicate
  dup.row <- which(df$sheet_name == the_sheet & 
                   df$round == 2 & 
                   df$respondant_name == respondent & 
                   df$measure == max_measure)

  # If such a row exists, duplicate and change measure to 12
  if (length(dup.row) > 0) {
    added.row <- df[dup.row, ]
    added.row$measure <- 12
    df <- rbind(df, added.row)
  }
return(df)
}

not_complte_respondents <- c("Bob Epsom", "David L", "David V", "Dean K", "Kylie Jo-Ma", "Rhiannon Hoy", "Saul H")
# Loop through respondents and add measure 12 where needed
for (i in not_complte_respondents) {
  c.df <- add_veteran_measure_twelve(c.df, i, this.sheet)
}

## Dont start at 0
### Dean K starts at 1, duplicate that row and add it in, editing measure to 0
dup.row <- which(c.df$sheet_name == this.sheet & 
                  c.df$round == 2 & 
                  c.df$respondant_name == "Dean K" & 
                  c.df$measure == 1)
added.row <- c.df[dup.row, ] # duplicate row
added.row$measure <- 0 # change measure to 0
c.df <- rbind(c.df, added.row) # insert row to df

```

```{r check_r2_veteran_trees}
ind_check <- ind_checker(c.df)
call_ind_mess(ind_check)
```

### Regeneration

```{r regeneration_curation}
this.sheet <- "Regen"
# curation where incomplete (see below check_r2_regeneration)
## Bob Epsom lowest measure is 1, duplicate that row and add it in, editing measure to 0
dup.row <- which(c.df$sheet_name == this.sheet & 
                  c.df$round == 2 & 
                  c.df$value == 0 & 
                  c.df$measure == 1 & 
                  c.df$respondant_name == "Bob Epsom") 
added.row <- c.df[dup.row, ] # duplicate row
added.row$measure <- 0 # change measure to 0
c.df <- rbind(c.df, added.row) # insert row to df
```

```{r check_r2_regeneration}
ind_check <- ind_checker(c.df)
call_ind_mess(ind_check)
```

### Herbivore damage

```{r herbivore_damage_curation}
this.sheet <- "Herbivore damage"
# curation where incomplete (see below check_r2_herbivore_damage)

```

```{r check_r2_herbivore_damage}
ind_check <- ind_checker(c.df)
call_ind_mess(ind_check)
```

### Tree health

```{r tree_health_curation}
this.sheet <- "Tree health"

# Bob Epsom min measure is 10, duplicate that row and add it in, editing measure to 0
dup.row <- which(c.df$sheet_name == this.sheet & 
                  c.df$round == 2 & 
                  c.df$value == 100 & 
                  c.df$measure == 10 & 
                  c.df$respondant_name == "Bob Epsom")
added.row <- c.df[dup.row, ] # duplicate row
added.row$measure <- 0 # change measure to 0
c.df <- rbind(c.df, added.row) # insert row to df


```

```{r check_r2_tree_health}
ind_check <- ind_checker(c.df)
call_ind_mess(ind_check)
```

### Ground flora

```{r ground_flora_curation}
this.sheet <- "Ground flora"
```

```{r check_r2_ground_flora}
ind_check <- ind_checker(c.df)
call_ind_mess(ind_check)
```

```{r temporary dataset}
c.df <- c.df 
```

### Horizontal complexity

```{r horizontal_complexity_curation}
this.sheet <- "Horizontal complexity"

# a lot have min measure 1, duplicate that row and add it in, editing measure to 0
respondents_with_issues <- c("Martin Hu", "Bob Epsom", "Dean K", "Kylie Jo-Ma", 
                            "Nick RB", "Sonia", "Mick Br", "Lou Ha", "Peter Lo")
for (respondent in respondents_with_issues) {
  dup.row <- which(c.df$sheet_name == this.sheet & 
                    c.df$round == 2 & 
                    c.df$value == 0 & 
                    c.df$measure == 1 & 
                    c.df$respondant_name == respondent) 
  added.row <- c.df[dup.row, ] # duplicate row
  added.row$measure <- 0 # change measure to 0
  c.df <- rbind(c.df, added.row) # insert row to df
}

# Mick Br has a measure 4 with value 100, but no measure 5, so duplicate that row and add it in, editing measure to 5
dup.row <- which(c.df$sheet_name == this.sheet & 
                  c.df$round == 2 & 
                  c.df$value == 100 & 
                  c.df$measure == 4 & 
                  c.df$respondant_name == "Mick Br")
added.row <- c.df[dup.row, ] # duplicate row
added.row$measure <- 5 # change measure to 5
c.df <- rbind(c.df, added.row) # insert row to df


# All with measure = 1 above value = 0 have their values of measure >= 1 rescaled so that value at measure 1 = 0
respondents_with_issues <- c.df %>%
  filter(sheet_name == this.sheet & 
         round == 2 & 
         measure == 1 & 
         value != 0) %>%
  select(respondant_name) %>%
  distinct() %>%
  pull(respondant_name)

for (respondent in respondents_with_issues) {
  # get value at measure 1 for this respondent
  value_at_measure_1 <- c.df %>%
    filter(sheet_name == this.sheet, 
           round == 2, 
           respondant_name == respondent, 
           measure == 1) %>%
    summarise(value = unique(value)) %>%
    pull(value)
  # rescale values where measure >= 1 for this respondent
  c.df$value[c.df$sheet_name == this.sheet & 
              c.df$round == 2 & 
              c.df$respondant_name == respondent & 
              c.df$measure >= 1] <- 
    (c.df$value[c.df$sheet_name == this.sheet & 
                c.df$round == 2 & 
                c.df$respondant_name == respondent & 
                c.df$measure >= 1] - value_at_measure_1)*
    (100 / (100 - value_at_measure_1))
}

```

```{r check_r2_horizontal_complexity}
ind_check <- ind_checker(c.df)
call_ind_mess(ind_check)
```

### Antropogenic damage

```{r anthropogenic_damage_curation}
this.sheet <- "Anthropogenic damage"

# Dean K has a measure 90 with value 0, but no measure 100, so duplicate that row and add it in, editing measure to 100
dup.row <- which(c.df$sheet_name == this.sheet & 
                  c.df$round == 2 & 
                  c.df$value == 0 & 
                  c.df$measure == 90 & 
                  c.df$respondant_name == "Dean K")
added.row <- c.df[dup.row, ] # duplicate row
added.row$measure <- 100 # change measure to 100
c.df <- rbind(c.df, added.row) # insert row to df

```

```{r check_r2_anthropogenic_damage}
ind_check <- ind_checker(c.df)
call_ind_mess(ind_check)
```

### Microhabitats

Microhabitats was not included within the Delphi metric, being added after the surveys. So the value function is based on the consensus of the expert opinion of EM, SH and MH.

</div>
:::


# Estimating value functions and generating lookup tables

Here, I go through each indicator, take each respondent's value function and:

1.  Interpolate data along that value function, so that every respondent's value function has the same number of data points (where required).
2.  Fit a model to the interpolated data from all respondents, using a GAM or qunatile GAM in most instances, estimating the "average" value function for that indicator.
3.  Use that model to predict values for each measure, generating a lookup table and figure of the value function.
4.  4\. Add a column to that look up table with the corresponding observations from the field-data, where necessary (i.e. when field data uses DOMIN score, but the value function used raw %).

```{r curation for interpolation and estimation}
c.df$value.dec = c.df$value/100 # a decimal version of value to work in the later binomial gams
```

```{r function-addpredictions_to_newdata}
add_gam_predictions <- function(model, newdata, scale_to_100 = TRUE) {
  # Predict from model
  pred <- predict(model, newdata = newdata, type = "link", se.fit = TRUE)
  
  # Add predictions to data
  newdata <- newdata %>%
    mutate(
      fit_link = pred$fit,
      se_link = pred$se.fit,
      fit_logit_lower = fit_link - 1.96 * se_link,
      fit_logit_upper = fit_link + 1.96 * se_link,
      fit_response = plogis(fit_link) * 100,
      lower_response = plogis(fit_logit_lower) * 100,
      upper_response = plogis(fit_logit_upper) * 100
    )
  
  # Optionally scale to 0–100
  if (scale_to_100) {
    rng <- range(newdata$fit_response, na.rm = TRUE)
    newdata <- newdata %>%
      mutate(
        value_plot = (fit_response - rng[1]) * (100 / (rng[2] - rng[1]))
      )
  }

  return(newdata)
}
```

```{r config for interpolation_of_vfs, warning=FALSE}
# interpolation of indivator value functions, using best fit model from linear, binom GLM, GAM, 

# set values for interpolation ----
# note dependency on sheet_name
interpolation_measurements <- list(
  "Tree age distribution" = unique(c.df$measure[c.df$sheet_name == "Tree age distribution"]) %>% sort(),
  "Native canopy percentage " = seq(from = 0, to = 100, by = 1),
  "Vertical structure" = unique(c.df$measure[c.df$sheet_name == "Vertical structure"]) %>% sort(),
  "N tree & shrub spp." = seq(from = 0, to = 100, by = 1),
  "Invasive plants % cover" = seq(from = 0, to = 100, by = 1),
  "Deadwood" = 0:max(c.df$measure[c.df$sheet_name == "Deadwood"]),
  "Veteran trees" = seq(from = 0, to = max(c.df$measure[c.df$sheet_name == "Veteran trees"] ), by = 0.1),
  # "Woodland extent" = seq(from = 0, to = 100, by = 1),
  "Regen" = 0:max(c.df$measure[c.df$sheet_name == "Regen"]),
  "Herbivore damage" = min(c.df$measure[c.df$sheet_name == "Herbivore damage"]):max(c.df$measure[c.df$sheet_name == "Herbivore damage"]),
  "Tree health" = seq(from = 0, to = 100, by = 1),
  "Ground flora" = seq(from = 0, to = 100, by = 1),
  "Horizontal complexity" = 0:5,
  "Anthropogenic damage" = seq(from = 0, to = 100, by = 1),
  "Microhabitats" = seq(from = 0, to = 100, by = 10)
)

# set fixed x - intercepts ----
x_intercept_fixed <- list(
  "Tree age distribution" = 1,
  "Native canopy percentage " = 0,
  "Vertical structure" = 1,
  "N tree & shrub spp." = 0,
  "Invasive plants % cover" = 0,
  "Deadwood" = 0,
  "Veteran trees" = 0,
  # "Woodland extent" = 0,
  "Regen" = 0,
  "Herbivore damage" = 0,
  "Tree health" = 0,
  "Ground flora" = 0,
  "Horizontal complexity" = 0,
  "Anthropogenic damage" = 0,
  "Microhabitats" = 0
)

```

```{r function-interpolation }
generate_plot_data <- function(this.sheet, this.round, c.df, interpolation_measurements) {
  
  # Step 1: Filter for sheet and round
  df_filtered <- c.df %>%
    filter(sheet_name == this.sheet, round == this.round) %>%
    arrange(respondant_name, measure)
  
  # Step 2: Check whether all respondents have the same measure series
  measure_check <- df_filtered %>%
    group_by(respondant_name) %>%
    summarise(measure_series = paste(measure, collapse = "|"), .groups = "drop") %>%
    pull(measure_series) %>%
    unique()
  
  # Step 3: Decide whether interpolation is needed
  interpolate <- length(measure_check) != 1
  
  if (interpolate) {
    message("❌ Interpolation needed - not all respondents have the same series of measure values for '", this.sheet, "'.")
  } else {
    message("✅ No interpolation needed - all respondents have the same series of measure values for '", this.sheet, "'.")
  }

  # Step 4: If no interpolation needed, return the existing data as-is
  if (!interpolate) {
      respondents <- unique(df_filtered$respondant_name)

      interpolated.df <- df_filtered %>%
      mutate(value = value.dec * 100) %>%
      select(respondant_name, measure, value)
  }
  
  # Step 5: Interpolate if needed (your original logic)
    if (interpolate) {
  respondents <- unique(df_filtered$respondant_name)
  meas <- interpolation_measurements[[this.sheet]]
  interpolated.df <- data.frame()
  
  for (resp in respondents) {
    sub <- df_filtered %>% filter(respondant_name == resp)
    if (nrow(sub) < 2 || diff(range(sub$value, na.rm = TRUE)) == 0) next
    
    response_scaled <- (sub$value - min(sub$value)) / (max(sub$value) - min(sub$value))
    dat <- data.frame(explanatory = sub$measure, response_scaled)
    
    # gam_linear <- gam(response_scaled ~ s(explanatory, bs = "cs", k = length(unique(sub$measure)) - 1),
    #                   data = dat, method = "REML")
    gam_binom <- gam(response_scaled ~ s(explanatory, bs = "cs", k = length(unique(sub$measure)) - 1),
                     family = binomial(link = "logit"), data = dat, method = "REML")
    
    best_model <- gam_binom#if (AIC(gam_linear) < AIC(gam_binom)) gam_linear else gam_binom
    
    newdat <- data.frame(explanatory = meas)
    preds <- predict(best_model, newdata = newdat, type = "response")
    preds <- (preds - min(preds)) / (max(preds) - min(preds))
    
    pred_df <- data.frame(
      respondant_name = resp,
      measure = meas,
      value = preds * 100
    )
    interpolated.df <- bind_rows(interpolated.df, pred_df)
  }
    }
  interpolated.df$value.dec <- interpolated.df$value / 100 # convert to decimal
  interpolated.df$respondant_name <- factor(interpolated.df$respondant_name, levels = respondents)
  return(interpolated.df)

}

interpolate_value_function <- function(this.sheet, this.round, c.df, interpolation_measurements) {
  # Get all respondents for this indicator and round
  respondents <- unique(c.df$respondant_name[c.df$sheet_name == this.sheet & c.df$round == this.round])
  
  # Measurements to interpolate over
  meas <- interpolation_measurements[[this.sheet]]
  
  # Prepare an empty data frame to store results
  interpolated.df <- data.frame()
  
  for (resp in respondents) {
    # Subset data for one respondent
    sub <- c.df %>%
      filter(sheet_name == this.sheet,
             round == this.round,
             respondant_name == resp)
    
    # Skip if insufficient or constant data
    if (nrow(sub) < 2 || diff(range(sub$value, na.rm = TRUE)) == 0) next
    
    # Scale response to 0–1
    response_scaled <- (sub$value - min(sub$value)) / (max(sub$value) - min(sub$value))
    dat <- data.frame(explanatory = sub$measure, response_scaled)
    
    # Fit two GAMs
    gam_linear <- gam(response_scaled ~ s(explanatory, bs = "cs", k = length(unique(sub$measure))-1), 
                      data = dat, method = "REML")
    gam_binom <- gam(response_scaled ~ s(explanatory, bs = "cs", k = length(unique(sub$measure))-1), 
                     family = binomial(link = "logit"), data = dat, method = "REML")
    
    # Select better model by AIC
    best_model <- if (AIC(gam_linear) < AIC(gam_binom)) gam_linear else gam_binom
    
    # Predict across range
    newdat <- data.frame(explanatory = meas)
    preds <- predict(best_model, newdata = newdat, type = "response")
    preds <- (preds - min(preds)) / (max(preds) - min(preds)) # scale to 0-1
    
    # Rescale to 0–100
    preds_rescaled <- (preds - min(preds)) * 100 / (max(preds) - min(preds))
    
    # Get static values from one of the rows
    template_row <- sub[1, , drop = FALSE]
    
    # Create output rows
    pred_df <- data.frame(
      indicator_name = template_row$indicator_name,
      respondant_name = resp,
      indicator_num = template_row$indicator_num,
      measure = meas,
      value = preds_rescaled,
      cert_val_funct = template_row$cert_val_funct,
      vf.sentance = template_row$vf.sentance,
      weight = template_row$weight,
      cert_weight = template_row$cert_weight,
      sheet_name = this.sheet,
      weight.name = template_row$weight.name,
      ind.axis.title = template_row$ind.axis.title,
      weight_rank = template_row$weight_rank,
      value.dec = preds / max(preds),
      round = this.round
    )
    
    # Bind to the full df
    interpolated.df <- bind_rows(interpolated.df, pred_df)
  }

  interpolated.df$value.dec <- interpolated.df$value / 100 # convert to decimal
  return(interpolated.df)
}
#function to check if every respondent has the same measures for this indicator
check_identical_measure_series <- function(df, this.sheet, this.round = 2) {
  # Filter the data for the given indicator sheet
  df_filtered <- df %>% 
    filter(sheet_name == this.sheet & round == this.round) %>%
    select(respondant_name, measure, value) %>%
    arrange(respondant_name, measure)
# Group by respondent and collapse the measure values into a single string
measure_strings <- df_filtered %>%
    group_by(respondant_name) %>%
    summarise(measure_series = paste(measure, collapse = "|"), .groups = "drop")

  # Check how many unique series there are
  unique_series <- unique(measure_strings$measure_series)
  
  if (length(unique_series) == 1) {
    message("✅ No interpolation needed - all respondents have the same series of measure values for '", this.sheet, "'.")
  } else {
    message("❌  Interpolation needed  - not all respondents have the same series of measure values for '", this.sheet, "'., interpolation needed")
  }
}
```

## Age structure

```{r age_structure_function_interpolation, warning=FALSE, message=FALSE}
this.sheet <- "Tree age distribution"
x.axis_title <- "N Tree Age Categories"
plot_data <- generate_plot_data(this.sheet, this.round, c.df, interpolation_measurements)

```

Estimated value using a binomial GAM.

```{r binomial_glm_age_structure, warning=FALSE, message=FALSE}

# binomial GAM for age structure
age_structure_gam <- gam(
  value.dec ~ s(measure, bs = "cs", k = 4)  ,
  data = plot_data,
  family = binomial(link = "logit")
)
# dummy data for prediction
newdata <- data.frame(
    measure = rep(unique(plot_data$measure), each = 1)#,
    #respondant_name = rep(unique(plot_data$respondant_name), times = length(unique(plot_data$measure)))
)

newdata <- add_gam_predictions(age_structure_gam, newdata, scale_to_100 = TRUE)

age_structure_predictions <- newdata %>%
  select(measure, value_plot) %>%
  rename(value = value_plot)

original_points <- c.df %>%
  filter(sheet_name == this.sheet, round == this.round)

ggplot() +
  # Original data points
  # geom_point(
  #   data = original_points,
  #   aes(x = measure, y = value, color = respondant_name),
  #   alpha = 0.8, size = 0.2
  # ) +
  # Respondent interpolated lines
  geom_line(
    data = original_points,
    aes(x = measure, y = value, color = respondant_name, group = respondant_name),
    alpha = 0.2, size = 0.6
  ) +
  # Fitted overall GAM line
  geom_line(
    data = newdata,
    aes(x = measure, y = value_plot),
    color = "black", size = 1.2
  ) +
  scale_color_manual(values = respondant_colours) +
  labs(
    x = x.axis_title,
    y = "Value",
    color = "Respondent",
    title = paste(this.sheet, "estimated value function")
  ) +
  theme_pubr()+
  theme(legend.position = "none") 


```

Look-up table

```{r age_structure_lookup_table, warning=FALSE}
age_vf_lookup <- newdata %>%
  select(measure, value_plot) %>%
    rename(!!x.axis_title := measure,
           value = value_plot) %>%
  #remove duplicates
  distinct() 

# save lookup table as csv
write.csv(age_vf_lookup, paste0("outputs\\lookup_tables\\", this.sheet, "_vf_lookup.csv"), row.names = FALSE)


datatable(
  age_vf_lookup,
  extensions = 'Buttons',
  options = list(
    dom = 'Btip',  
    buttons = c('copy', 'csv')
  )
)

```

## Native canopy percentage

```{r native_canopy_percentage_function_interpolation, warning=FALSE}
this.sheet <- "Native canopy percentage "
x.axis_title <- "Native Canopy Percentage"
plot_data <- generate_plot_data(this.sheet, this.round, c.df, interpolation_measurements)
```

Estimated value using a linear GAM.

```{r native_canopy_percentage_model, warning=FALSE}
# test binomial and linear GAMs
## binomial GAM 
native_canopy_gam1 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 10)  ,
  data = plot_data,
  family = binomial(link = "logit")
)
## linear GAM
native_canopy_gam2 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 10)  ,
  data = plot_data,
  family = gaussian(link = "identity")
)
# Compare AIC
aic_native_canopy <- AIC(native_canopy_gam1, native_canopy_gam2)
# Choose the best model based on AIC
native_canopy_gam <- native_canopy_gam2 ####!!!!!

# dummy data for prediction
newdata <- data.frame(
    measure = rep(unique(plot_data$measure), each = 1)#,
    #respondant_name = rep(unique(plot_data$respondant_name), times = length(unique(plot_data$measure)))
)
newdata <- add_gam_predictions(native_canopy_gam, newdata, scale_to_100 = TRUE)

native_canopy_predictions <- newdata %>%
  select(measure, value_plot) %>%
  rename(value = value_plot)
```

```{r native_canopy_percentage_plot, warning=FALSE}

# 5. Plot
original_points <- c.df %>%
  filter(sheet_name == this.sheet, round == this.round)

ggplot() +
  # Original data points
  # geom_point(
  #   data = original_points,
  #   aes(x = measure, y = value, color = respondant_name),
  #   alpha = 0.8, size = 0.2
  # ) +
  # Respondent interpolated lines
  geom_line(
    data = original_points,
    aes(x = measure, y = value, color = respondant_name, group = respondant_name),
    alpha = 0.2, size = 0.6
  ) +
  # Fitted overall GAM line
  geom_line(
    data = newdata,
    aes(x = measure, y = value_plot),
    color = "black", size = 1.2
  ) +
  scale_color_manual(values = respondant_colours) +
  labs(
    x = x.axis_title,
    y = "Value",
    color = "Respondent",
    title = paste(this.sheet, "estimated value function")
  ) +
  theme_pubr()+
  theme(legend.position = "none") 

```

Lookup table

```{r native_canopy_percentage_lookup_table, warning=FALSE}
native_canopy_vf_lookup <- newdata %>%
  select(measure, value_plot) %>%
    rename(!!x.axis_title := measure,
           value = value_plot) %>%
  #remove duplicates
  distinct()
# save lookup table as csv
write.csv(native_canopy_vf_lookup, paste0("outputs\\lookup_tables\\", this.sheet, "_vf_lookup.csv"), row.names = FALSE)
datatable(
  native_canopy_vf_lookup %>% 
    mutate(value = round(value, 2)),
  extensions = 'Buttons',
  options = list(
    dom = 'Btip',  
    buttons = c('copy', 'csv')
  )
)
```

## Vertical structure

```{r vertical_structure_function_interpolation, warning=FALSE}
this.sheet <- "Vertical structure"
x.axis_title <- "N Vertical Structure Categories" 
plot_data <- generate_plot_data(this.sheet, this.round, c.df, interpolation_measurements)
```

Estimated value using a linear GAM.

```{r vertical_structure_model, warning=FALSE}
# test binomial and linear GAMs
## binomial GAM
vertical_structure_gam1 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 4)  ,
  data = plot_data,
  family = binomial(link = "logit")
)
## linear GAM
vertical_structure_gam2 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 4)  ,
  data = plot_data,
  family = gaussian(link = "identity")
)
# Compare AIC
aic_vertical_structure <- AIC(vertical_structure_gam1, vertical_structure_gam2)
# Choose the best model based on AIC
vertical_structure_gam <- vertical_structure_gam2 ####!!!!!
# dummy data for prediction
newdata <- data.frame(
    measure = rep(unique(plot_data$measure), each = 1)#,
    #respondant_name = rep(unique(plot_data$respondant_name), times = length(unique(plot_data$measure)))
)
newdata <- add_gam_predictions(vertical_structure_gam, newdata, scale_to_100 = TRUE)

vertical_structure_predictions <- newdata %>%
  select(measure, value_plot) %>%
  rename(value = value_plot)

```

```{r vertical_structure_plot, warning=FALSE}
# 5. Plot
original_points <- c.df %>%
  filter(sheet_name == this.sheet, round == this.round)
ggplot() +
  # Original data points
  # geom_point(
  #   data = original_points,
  #   aes(x = measure, y = value, color = respondant_name),
  #   alpha = 0.8, size = 0.2
  # ) +
  # Respondent interpolated lines
  geom_line(
    data = original_points,
    aes(x = measure, y = value, color = respondant_name, group = respondant_name),
    alpha = 0.2, size = 0.6
  ) +
  # Fitted overall GAM line
  geom_line(
    data = newdata,
    aes(x = measure, y = value_plot),
    color = "black", size = 1.2
  ) +
  scale_color_manual(values = respondant_colours) +
  labs(
    x = x.axis_title,
    y = "Value",
    color = "Respondent",
    title = paste(this.sheet, "estimated value function" )
  ) +
  theme_pubr()+
  theme(legend.position = "none")
```

Lookup table

```{r vertical_structure_lookup_table, warning=FALSE}
vertical_structure_vf_lookup <- newdata %>%
  select(measure, value_plot) %>%
    rename(!!x.axis_title := measure,
           value = value_plot) %>%
  #remove duplicates
  distinct()
# save lookup table as csv
write.csv(vertical_structure_vf_lookup, paste0("outputs\\lookup_tables\\", this.sheet, "_vf_lookup.csv"), row.names = FALSE)
datatable(
  vertical_structure_vf_lookup %>% 
    mutate(value = round(value, 2)),
  extensions = 'Buttons',
  options = list(
    dom = 'Btip',  
    buttons = c('copy', 'csv')
  )
)
```

## Tree species richness

```{r n_tree_shrub_spp_function_interpolation, warning=FALSE, message=FALSE}
this.sheet <- "N tree & shrub spp."
x.axis_title <- "Proportion of appropriate Tree & Shrub Species"
plot_data <- generate_plot_data(this.sheet, this.round, c.df, interpolation_measurements)
```

Estimated value using a linear GAM.

```{r n_tree_shrub_spp_model, warning=FALSE, message=FALSE}
# test binomial and linear GAMs
## binomial GAM
n_tree_shrub_spp_gam1 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 10)  ,
  data = plot_data,
  family = binomial(link = "logit")
)
## linear GAM
n_tree_shrub_spp_gam2 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 10)  ,
  data = plot_data,
  family = gaussian(link = "identity")
)
# Compare AIC
aic_n_tree_shrub_spp <- AIC(n_tree_shrub_spp_gam1, n_tree_shrub_spp_gam2)
# Choose the best model based on AIC
n_tree_shrub_spp_gam <- n_tree_shrub_spp_gam2 ####!!!!!
# dummy data for prediction
newdata <- data.frame(
    measure = rep(unique(plot_data$measure), each = 1)#,
    #respondant_name = rep(unique(plot_data$respondant_name), times = length(unique(plot_data$measure)))
)
newdata <- add_gam_predictions(n_tree_shrub_spp_gam, newdata, scale_to_100 = TRUE)
n_tree_shrub_spp_predictions <- newdata %>%
  select(measure, value_plot) %>%
  rename(value = value_plot)

```

```{r n_tree_shrub_spp_plot, warning=FALSE, message=FALSE}
# 5. Plot
original_points <- c.df %>%
  filter(sheet_name == this.sheet, round == this.round)
ggplot() +
  # Original data points
  # geom_point(
  #   data = original_points,
  #   aes(x = measure, y = value, color = respondant_name),
  #   alpha = 0.8, size = 0.2
  # ) +
  # Respondent interpolated lines
  geom_line(
    data = original_points,
    aes(x = measure, y = value, color = respondant_name, group = respondant_name),
    alpha = 0.2, size = 0.6
  ) +
  # Fitted overall GAM line
  geom_line(
    data = newdata,
    aes(x = measure, y = value_plot),
    color = "black", size = 1.2
  ) +
  scale_color_manual(values = respondant_colours) +
  labs(
    x = x.axis_title,
    y = "Value",
    color = "Respondent",
    title = paste(this.sheet, "estimated value function")
  ) +
  theme_pubr()+
  theme(legend.position = "none")
```

Look-up table

```{r n_tree_shrub_spp_lookup_table, warning=FALSE, message=FALSE}
n_tree_shrub_spp_vf_lookup <- newdata %>%
  select(measure, value_plot) %>%
    rename(!!x.axis_title := measure,
           value = value_plot) %>%
  #remove duplicates
  distinct()
# save lookup table as csv
write.csv(n_tree_shrub_spp_vf_lookup, paste0("outputs\\lookup_tables\\", this.sheet, "_vf_lookup.csv"), row.names = FALSE)
datatable(
  n_tree_shrub_spp_vf_lookup %>% 
    mutate(value = round(value, 2)),
  extensions = 'Buttons',
  options = list(
    dom = 'Btip',  
    buttons = c('copy', 'csv')
  )
)
```

## Invasive plants

```{r invasive_plants_function_interpolation, warning=FALSE}
this.sheet <- "Invasive plants % cover"
x.axis_title <- "Invasive Plants % Cover"
plot_data <- generate_plot_data(this.sheet, this.round, c.df, interpolation_measurements)
```

I first estimated value using a linear GAM, but it was influenced a lot by outlier value functions, because it kind of represents the mean value at each measure. So, experimented with a quantile GAM, tuning the quantile manually to hit the approximate median of values at key measurements along the function (10, 20 and 30% invasive cover). Nifty, if a little hack!

```{r invasive_plants_model, warning=FALSE, messge=FALSE}
# test binomial and linear GAMs
## binomial GAM
invasive_plants_gam1 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 10)  ,
  data = plot_data,
  family = binomial(link = "logit")
)
## linear GAM
invasive_plants_gam2 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 10)  ,
  data = plot_data,
  family = gaussian(link = "identity")
)
## Quantile GAM
invasive_plants_gam3 <- qgam(
  value.dec ~ s(measure, bs = "cs", k = 10),
  data = plot_data,
  qu = 0.28
  )
# Compare AIC
aic_invasive_plants <- AIC(invasive_plants_gam1, invasive_plants_gam2, invasive_plants_gam3)
# Choose the best model based on AIC
invasive_plants_gam <- invasive_plants_gam3 ####!!!!!
# dummy data for prediction
newdata <- data.frame(
    measure = rep(unique(plot_data$measure), each = 1)#,
    #respondant_name = rep(unique(plot_data$respondant_name), times = length(unique(plot_data$measure)))
)
newdata <- add_gam_predictions(invasive_plants_gam, newdata, scale_to_100 = TRUE)

invasive_plants_predictions <- newdata %>%
  select(measure, value_plot) %>%
  rename(value = value_plot)

```

```{r invasive_plants_plot, warning=FALSE}
# original points
original_points <- c.df %>%
  filter(sheet_name == this.sheet, round == this.round) 
# add prediction
original_points <- original_points %>%
  mutate(pred_value = predict(
    invasive_plants_gam, 
    newdata = data.frame(measure = measure), 
    type = "response"
    ))
range_pred <- range(original_points$pred_value, na.rm = TRUE)
original_points$pred_value <- (original_points$pred_value - range_pred[1]) * 100 / (range_pred[2] - range_pred[1]) 
test_measures_considered <-  c(10, 20, 30)
  
# pull pred down? - is value bigger than prediction?
test_pred <- original_points [original_points$measure %in%test_measures_considered, ] # where prediction is less than value
# table(test_pred$pred_value > test_pred$value)

ggplot() +
  # Original data points
  # geom_point(
  #   data = original_points,
  #   aes(x = measure, y = value, color = respondant_name),
  #   alpha = 0.8, size = 0.2
  # ) +
  # Respondent interpolated lines
  geom_line(
    data = original_points,
    aes(x = measure, y = value, color = respondant_name, group = respondant_name),
    alpha = 0.2, size = 0.6
  ) +
  # Fitted overall GAM line
  geom_line(
    data = newdata,
    aes(x = measure, y = value_plot),
    color = "black", size = 1.2
  ) +
  scale_color_manual(values = respondant_colours) +
  labs(
    x = x.axis_title,
    y = "Value",
    color = "Respondent",
    title = paste(this.sheet, "estimated value function")
  ) +
  theme_pubr()+
  theme(legend.position = "none")
```

Lookup table

```{r f-make_domin_lookup, warning=FALSE}
make_domin_lookup <- function(value_data, summary_func = median) {
  # Define fixed DOMIN ranges
  domin_ranges <- list(
    "0% Absent"                  = function(x) x == 0,
    "< 4% ( Rare - 1 Few individuals)"         = function(x) x < 4,
    "< 4% ( Rare - 2 Several individuals)"     = function(x) x < 4,
    "< 4% ( Rare - 3 Many individuals)"        = function(x) x < 4,
    "4 - 10% (Rare - 4)"           = function(x) x >= 4 & x < 10,
    "10 – 25% ( Occasional - 5)"          = function(x) x >= 10 & x < 25,
    "26 – 33% (Frequent - 6)"          = function(x) x >= 25 & x < 33,
    "33 – 50% (Frequent - 7)"          = function(x) x >= 33 & x < 50,
    "50 – 75% (Abundant - 8)"          = function(x) x >= 50 & x < 75,
    "75 – 90% (Dominant - 9)"          = function(x) x >= 75 & x < 90,
    "90 – 100% (Dominant - 10)"        = function(x) x >= 90
  )

  # Apply the summary function (default = median) to each DOMIN class range
  domin_values <- sapply(names(domin_ranges), function(class) {
    range_filter <- domin_ranges[[class]]
    values <- value_data$value_plot[range_filter(value_data$measure)]
    summary_func(values, na.rm = TRUE)
  })

  # Return lookup table
  data.frame(
    domin = names(domin_ranges),
    value = domin_values,
    row.names = NULL
  )
}
```

```{r invasive_plants_lookup_table, warning=FALSE}
invasive_plants_vf_lookup <- newdata %>%
  select(measure, value_plot) %>%
  mutate(value_plot = round(value_plot, 2)) 

# make an alternative DOMIN class lookup table, taking the median value across the range of measurements
invasive_plants_vf_lookup_domin <- make_domin_lookup(invasive_plants_vf_lookup, summary_func = median) %>% 
  mutate(value = round(value, 2)) 


invasive_plants_vf_lookup <- invasive_plants_vf_lookup %>% 
    rename(!!x.axis_title := measure,
           value = value_plot) %>%
  #remove duplicates
  distinct()

# save lookup table as csv
write.csv(invasive_plants_vf_lookup_domin, paste0("outputs\\lookup_tables\\", this.sheet, "_vf_lookup_domin.csv"), row.names = FALSE)
write.csv(invasive_plants_vf_lookup, paste0("outputs\\lookup_tables\\", this.sheet, "_vf_lookup.csv"), row.names = FALSE)
datatable(
  invasive_plants_vf_lookup,
    rownames = FALSE,
  extensions = 'Buttons',
  options = list(
    dom = 'Btip',  
    buttons = c('copy', 'csv')
  )
)
```

DOMIN class lookup table

```{r invasive_plants_lookup_table_domin, warning=FALSE}
datatable(
  invasive_plants_vf_lookup_domin,
  rownames = FALSE,
  extensions = 'Buttons',
  options = list(
    dom = 'Btip',  
    buttons = c('copy', 'csv')
  )
)
```

## Deadwood

```{r deadwood_function_interpolation, warning=FALSE}
this.sheet <- "Deadwood"
x.axis_title <- "N Deadwood Categories"
plot_data <- generate_plot_data(this.sheet, this.round, c.df, interpolation_measurements)
```

Estimated value using a linear GAM.

```{r deadwood_model, warning=FALSE, message=FALSE}
# test binomial and linear GAMs
## binomial GAM
deadwood_gam1 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 10)  ,
  data = plot_data,
  family = binomial(link = "logit")
)
## linear GAM
deadwood_gam2 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 10)  ,
  data = plot_data,
  family = gaussian(link = "identity")
)
# Compare AIC
aic_deadwood <- AIC(deadwood_gam1, deadwood_gam2)
# Choose the best model based on AIC
deadwood_gam <- deadwood_gam2 ####!!!!!
# dummy data for prediction
newdata <- data.frame(
    measure = rep(unique(plot_data$measure), each = 1)#,
    #respondant_name = rep(unique(plot_data$respondant_name), times = length(unique(plot_data$measure)))
)
newdata <- add_gam_predictions(deadwood_gam, newdata, scale_to_100 = TRUE)

deadwood_predictions <- newdata %>%
  select(measure, value_plot) %>%
  rename(value = value_plot)

```

```{r deadwood_plot, warning=FALSE}
# 5. Plot
original_points <- c.df %>%
  filter(sheet_name == this.sheet, round == this.round)
ggplot() +
  # Original data points
  # geom_point(
  #   data = original_points,
  #   aes(x = measure, y = value, color = respondant_name),
  #   alpha = 0.8, size = 0.2
  # ) +
  # Respondent interpolated lines
  geom_line(
    data = original_points,
    aes(x = measure, y = value, color = respondant_name, group = respondant_name),
    alpha = 0.2, size = 0.6
  ) +
  # Fitted overall GAM line
  geom_line(
    data = newdata,
    aes(x = measure, y = value_plot),
    color = "black", size = 1.2
  ) +
  scale_color_manual(values = respondant_colours) +
  labs(
    x = x.axis_title,
    y = "Value",
    color = "Respondent",
    title = paste(this.sheet, "estimated value function")
  ) +
  theme_pubr()+
  theme(legend.position = "none")
```

Lookup table

```{r deadwood_lookup_table, warning=FALSE}
deadwood_vf_lookup <- newdata %>%
  select(measure, value_plot) %>%
    rename(!!x.axis_title := measure,
           value = value_plot) %>%
  #remove duplicates
  distinct()
# save lookup table as csv
write.csv(deadwood_vf_lookup, paste0("outputs\\lookup_tables\\", this.sheet, "_vf_lookup.csv"), row.names = FALSE)
datatable(
  deadwood_vf_lookup %>% 
    mutate(value = round(value, 2)), # round to 2 decimal places
  rownames = FALSE,
  extensions = 'Buttons',
  options = list(
    dom = 'Btip',  
    buttons = c('copy', 'csv')
  )
)
```

## Veteran trees

```{r veteran_trees_function_interpolation, warning=FALSE}
this.sheet <- "Veteran trees"
x.axis_title <- "N Veteran Trees per ha"
plot_data <- generate_plot_data(this.sheet, this.round, c.df, interpolation_measurements)
```

Initially estimated value using a linear GAM, but influenced too much by respondents who required many AVTs for good condition. So, as with invasive plants indicator above, I experimented with a quantile GAM, tuning the quantile manually to hit the approximate median of values at key measurements along the function (1, 2 and 3 AVTs per ha). This means, at least at these points, the value function is aproximately in the middle of the estimations from the respondents.

```{r veteran_trees_model, warning=FALSE, message=FALSE}
# test binomial and linear GAMs
## binomial GAM
veteran_trees_gam1 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 10)  ,
  data = plot_data,
  family = binomial(link = "logit")
)
## linear GAM
veteran_trees_gam2 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 10)  ,
  data = plot_data,
  family = gaussian(link = "identity")
)
## Quantile GAM
veteran_trees_gam3 <- qgam(
  value.dec ~ s(measure, bs = "cs", k = 10),
  data = plot_data,
  qu = 0.81
)

# Compare AIC
aic_veteran_trees <- AIC(veteran_trees_gam1, veteran_trees_gam2, veteran_trees_gam3)
# Choose the best model based on AIC
veteran_trees_gam <- veteran_trees_gam3 ####!!!!!
# dummy data for prediction
newdata <- data.frame(
    measure = rep(unique(plot_data$measure), each = 1)#,
    #respondant_name = rep(unique(plot_data$respondant_name), times = length(unique(plot_data$measure)))
)
newdata <- add_gam_predictions(veteran_trees_gam, newdata, scale_to_100 = TRUE)

veteran_trees_predictions <- newdata %>%
  select(measure, value_plot) %>%
  rename(value = value_plot)

# original points
original_points <- c.df %>%
  filter(sheet_name == this.sheet, round == this.round) 
# add prediction
original_points <- original_points %>%
  mutate(pred_value = predict(
    veteran_trees_gam, 
    newdata = data.frame(measure = measure), 
    type = "response"
    ))
range_pred <- range(original_points$pred_value, na.rm = TRUE)
original_points$pred_value <- (original_points$pred_value - range_pred[1]) * 100 / (range_pred[2] - range_pred[1]) 
test_measures_considered <-  1:5
  
# pull pred down? - is value bigger than prediction?
test_pred <- original_points [original_points$measure %in%test_measures_considered, ] # where prediction is less than value
# table(test_pred$pred_value > test_pred$value)

# Plot
ggplot() +
  # Original data points
  # geom_point(
  #   data = original_points,
  #   aes(x = measure, y = value, color = respondant_name),
  #   alpha = 0.8, size = 0.2
  # ) +
  # Respondent interpolated lines
  geom_line(
    data = original_points,
    aes(x = measure, y = value, color = respondant_name, group = respondant_name),
    alpha = 0.2, size = 0.6
  ) +
  # Fitted overall GAM line
  geom_line(
    data = newdata,
    aes(x = measure, y = value_plot),
    color = "black", size = 1.2
  ) +
  scale_color_manual(values = respondant_colours) +
  labs(
    x = x.axis_title,
    y = "Value",
    color = "Respondent",
    title = paste(this.sheet, "estimated value function")
  ) +
  theme_pubr()+
  theme(legend.position = "none")
```

Lookup table

```{r veteran_trees_lookup_table, warning=FALSE}
veteran_trees_vf_lookup <- newdata %>%
  select(measure, value_plot) %>%
  mutate(measure = round(measure, 2)) %>% 
  rename(!!x.axis_title := measure,
           value = value_plot) %>%
  #remove duplicates
  distinct()
# save lookup table as csv
write.csv(veteran_trees_vf_lookup, paste0("outputs\\lookup_tables\\", this.sheet, "_vf_lookup.csv"), row.names = FALSE)
datatable(
  veteran_trees_vf_lookup %>% 
    mutate(value = round(value, 2)), # round to 2 decimal places
  rownames = FALSE,
  extensions = 'Buttons',
  options = list(
    dom = 'Btip',  
    buttons = c('copy', 'csv')
  )
)
```

## Regen of trees & shrubs

```{r regen_function_interpolation, warning=FALSE}
this.sheet <- "Regen"
x.axis_title <- "N Regen classes present"
plot_data <- generate_plot_data(this.sheet, this.round, c.df, interpolation_measurements)
```

```{r regen_model, warning=FALSE, message=FALSE}
# test binomial and linear GAMs
## binomial GAM
regen_gam1 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 3)  ,
  data = plot_data,
  family = binomial(link = "logit")
)
## linear GAM
regen_gam2 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 3)  ,
  data = plot_data,
  family = gaussian(link = "identity")
)
# Compare AIC
aic_regen <- AIC(regen_gam1, regen_gam2)
# Choose the best model based on AIC
regen_gam <- regen_gam2 ####!!!!!
# dummy data for prediction
newdata <- data.frame(
    measure = rep(unique(plot_data$measure), each = 1)#,
    #respondant_name = rep(unique(plot_data$respondant_name), times = length(unique(plot_data$measure)))
)
newdata <- add_gam_predictions(regen_gam, newdata, scale_to_100 = TRUE)

regen_predictions <- newdata %>%
  select(measure, value_plot) %>%
  rename(value = value_plot)

# original points
original_points <- c.df %>%
  filter(sheet_name == this.sheet, round == this.round) 
# add prediction
original_points <- original_points %>%
  mutate(pred_value = predict(
    regen_gam, 
    newdata = data.frame(measure = measure), 
    type = "response"
    ))
range_pred <- range(original_points$pred_value, na.rm = TRUE)
original_points$pred_value <- (original_points$pred_value - range_pred[1]) * 100 / (range_pred[2] - range_pred[1]) 
test_measures_considered <-  1:3
  
# pull pred down? - is value bigger than prediction?
test_pred <- original_points [original_points$measure %in%test_measures_considered, ] # where prediction is less than value
# table(test_pred$pred_value > test_pred$value)


```

```{r regen_plot, warning=FALSE}
# 5. Plot
original_points <- c.df %>%
  filter(sheet_name == this.sheet, round == this.round)
ggplot() +
  # Original data points
  # geom_point(
  #   data = original_points,
  #   aes(x = measure, y = value, color = respondant_name),
  #   alpha = 0.8, size = 0.2
  # ) +
  # Respondent interpolated lines
  geom_line(
    data = original_points,
    aes(x = measure, y = value, color = respondant_name, group = respondant_name),
    alpha = 0.2, size = 0.6
  ) +
  # Fitted overall GAM line
  geom_line(
    data = newdata,
    aes(x = measure, y = value_plot),
    color = "black", size = 1.2
  ) +
  scale_color_manual(values = respondant_colours) +
  labs(
    x = x.axis_title,
    y = "Value",
    color = "Respondent",
    title = paste(this.sheet, "estimated value function")
  ) +
  theme_pubr()+
  theme(legend.position = "none")
```

Lookup table

```{r regen_lookup_table, warning=FALSE}
regen_vf_lookup <- newdata %>%
  select(measure, value_plot) %>%
  mutate(measure = round(measure, 2)) %>% 
  rename(!!x.axis_title := measure,
           value = value_plot) %>%
  #remove duplicates
  distinct()
# save lookup table as csv
write.csv(regen_vf_lookup, paste0("outputs\\lookup_tables\\", this.sheet, "_vf_lookup.csv"), row.names = FALSE)
datatable(
  regen_vf_lookup %>% 
    mutate(value = round(value, 2)), # round to 2 decimal places
  rownames = FALSE,
  extensions = 'Buttons',
  options = list(
    dom = 'Btip',  
    buttons = c('copy', 'csv')
  )
)
```

## Herbivore damage

```{r herbivore_damage_function_interpolation, warning=FALSE}
this.sheet <- "Herbivore damage"
x.axis_title <- "Damage class: potential 12-month growth removed"
plot_data <- generate_plot_data(this.sheet, this.round, c.df, interpolation_measurements)
```

Originally estimated value using a linear GAM, but it was influenced too much by VFs that improved condition very quickly from the highest herbivore impact. So, as with invasive plants and regen indicators above, I experimented with a quantile GAM, tuning the quantile manually to hit the approximate median of values at key measurements along the function (Classes 1, 2 and 3). This means, at least at these points, the value function is aproximately in the middle of the estimations from the respondents.

```{r herbivore_damage_model, warning=FALSE, message=FALSE}
# test binomial and linear GAMs
## binomial GAM
herbivore_damage_gam1 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 5)  ,
  data = plot_data,
  family = binomial(link = "logit")
)
## linear GAM
herbivore_damage_gam2 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 5)  ,
  data = plot_data,
  family = gaussian(link = "identity")
)
## Quantile GAM
herbivore_damage_gam3 <- qgam(
  value.dec ~ s(measure, bs = "cs", k = 5),
  data = plot_data,
  qu = 0.49
)

# Compare AIC
aic_herbivore_damage <- AIC(herbivore_damage_gam1, herbivore_damage_gam2, herbivore_damage_gam3)
# Choose the best model based on AIC
herbivore_damage_gam <- herbivore_damage_gam3 ####!!!!!
# dummy data for prediction
newdata <- data.frame(
    measure = rep(unique(plot_data$measure), each = 1)#,
    #respondant_name = rep(unique(plot_data$respondant_name), times = length(unique(plot_data$measure)))
)
newdata <- add_gam_predictions(herbivore_damage_gam, newdata, scale_to_100 = TRUE)

herbivore_damage_predictions <- newdata %>%
  select(measure, value_plot) %>%
  rename(value = value_plot)

# original points
original_points <- c.df %>%
  filter(sheet_name == this.sheet, round == this.round) 
# add prediction
original_points <- original_points %>%
  mutate(pred_value = predict(
    herbivore_damage_gam, 
    newdata = data.frame(measure = measure), 
    type = "response"
    ))
range_pred <- range(original_points$pred_value, na.rm = TRUE)
original_points$pred_value <- (original_points$pred_value - range_pred[1]) * 100 / (range_pred[2] - range_pred[1]) 
test_measures_considered <-  1:3
  
# pull pred down? - is value bigger than prediction?
test_pred <- original_points [original_points$measure %in%test_measures_considered, ] # where prediction is less than value
# table(test_pred$pred_value > test_pred$value)


ggplot() +
  # Original data points
  # geom_point(
  #   data = original_points,
  #   aes(x = measure, y = value, color = respondant_name),
  #   alpha = 0.8, size = 0.2
  # ) +
  # Respondent interpolated lines
  geom_line(
    data = original_points,
    aes(x = measure, y = value, color = respondant_name, group = respondant_name),
    alpha = 0.2, size = 0.6
  ) +
  # Fitted overall GAM line
  geom_line(
    data = newdata,
    aes(x = measure, y = value_plot),
    color = "black", size = 1.2
  ) +
  scale_color_manual(values = respondant_colours) +
  labs(
    x = x.axis_title,
    y = "Value",
    color = "Respondent",
    title = paste(this.sheet, "estimated value function")
  ) +
  theme_pubr()+
  theme(legend.position = "none")
```

Lookup table

```{r herbivore_damage_lookup_table}
herbivore_damage_vf_lookup <- newdata %>%
  select(measure, value_plot) %>%
  mutate(measure = round(measure, 2)) 

herbivore_damage_vf_lookup$measure_alt <- factor(
  herbivore_damage_vf_lookup$measure,
  levels = 0:4,
  labels = c("Negligable damage: <25% of palatable",
             "Low damage: 25-75% of palatable", 
             "Moderate damage: >75% of palatable and <25% unpalatable", 
             "High damage: >75% of palatable and 25-75%% unpalatable",
             "Very high damage: >75% of palatable and >75% unpalatable")
)


herbivore_damage_vf_lookup <- herbivore_damage_vf_lookup %>% 
  rename('Potential 12-month growth removed'= measure,
           value = value_plot) %>%
  select(measure_alt, value) %>%
  rename(!!x.axis_title := measure_alt) %>%
  #remove duplicates
  distinct()
# save lookup table as csv
write.csv(herbivore_damage_vf_lookup, paste0("outputs\\lookup_tables\\", this.sheet, "_vf_lookup.csv"), row.names = FALSE)
datatable(
  herbivore_damage_vf_lookup %>% 
    mutate(value = round(value, 2)), # round to 2 decimal places
  rownames = FALSE,
  extensions = 'Buttons',
  options = list(
    dom = 'Btip',  
    buttons = c('copy', 'csv')
  )
)
```

## Tree health

```{r tree_health_function_interpolation, warning=FALSE}
this.sheet <- "Tree health"
x.axis_title <- "Dieback or sudden mortality (%)"
plot_data <- generate_plot_data(this.sheet, this.round, c.df, interpolation_measurements)
```

Fitted model using a quantile GAM, tuned to hit the approximate median of values at key measurements along the function (10, 20, 30 and 40% dieback or mortality). This means, at least at these measures, the value function is aproximately in the middle of the respondent estimation data.

I think that bootstrapping a median and fitting a spline to that would be better, but I don't have the time to do that now.

```{r tree_health_model, warning=FALSE, message=FALSE}
# test binomial and linear GAMs
## binomial GAM
tree_health_gam1 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 10)  ,
  data = plot_data,
  family = binomial(link = "logit")
)
## linear GAM
tree_health_gam2 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 10)  ,
  data = plot_data,
  family = gaussian(link = "identity")
)
## Quantile GAM
tree_health_gam3 <- qgam(
  value.dec ~ s(measure, bs = "cs", k = 10),
  data = plot_data,
  qu = 0.33
)
# Compare AIC
aic_tree_health <- AIC(tree_health_gam1, tree_health_gam2, tree_health_gam3)
# Choose the best model based on AIC
tree_health_gam <- tree_health_gam3 ####!!!!!
# dummy data for prediction
newdata <- data.frame(
    measure = rep(unique(plot_data$measure), each = 1)#,
    #respondant_name = rep(unique(plot_data$respondant_name), times = length(unique(plot_data$measure)))
)
newdata <- add_gam_predictions(tree_health_gam, newdata, scale_to_100 = TRUE)

tree_health_predictions <- newdata %>%
  select(measure, value_plot) %>%
  rename(value = value_plot)

# original points
original_points <- c.df %>%
  filter(sheet_name == this.sheet, round == this.round)
# add prediction
original_points <- original_points %>%
  mutate(pred_value = predict(
    tree_health_gam, 
    newdata = data.frame(measure = measure), 
    type = "response"
    ))
range_pred <- range(original_points$pred_value, na.rm = TRUE)
original_points$pred_value <- (original_points$pred_value - range_pred[1]) * 100 / (range_pred[2] - range_pred[1])
test_measures_considered <-  c(10, 20, 30, 40)
# pull pred down? - is value bigger than prediction?
test_pred <- original_points [original_points$measure %in%test_measures_considered, ] # where prediction is less than value
# table(test_pred$pred_value > test_pred$value)

ggplot() +
  # Original data points
  # geom_point(
  #   data = original_points,
  #   aes(x = measure, y = value, color = respondant_name),
  #   alpha = 0.8, size = 0.2
  # ) +
  # Respondent interpolated lines
  geom_line(
    data = original_points,
    aes(x = measure, y = value, color = respondant_name, group = respondant_name),
    alpha = 0.2, size = 0.6
  ) +
  # Fitted overall GAM line
  geom_line(
    data = newdata,
    aes(x = measure, y = value_plot),
    color = "black", size = 1.2
  ) +
  scale_color_manual(values = respondant_colours) +
  labs(
    x = x.axis_title,
    y = "Value",
    color = "Respondent",
    title = paste(this.sheet, "estimated value function")
  ) +
  theme_pubr()+
  theme(legend.position = "none")
```

Lookup table

```{r tree_health_lookup_table, warning=FALSE}
tree_health_vf_lookup <- newdata %>%
  select(measure, value_plot) %>%
  mutate(measure = round(measure, 2))

# make an alternative DOMIN class lookup table, taking the median value across the range of measurements
tree_health_vf_lookup_domin <- make_domin_lookup(tree_health_vf_lookup, summary_func = median) %>% 
  mutate(value = round(value, 2))

tree_health_vf_lookup <- tree_health_vf_lookup %>% 
  rename(!!x.axis_title := measure,
           value = value_plot) %>%
  #remove duplicates
  distinct()
# save lookup table as csv
write.csv(tree_health_vf_lookup_domin, paste0("outputs\\lookup_tables\\", this.sheet, "_vf_lookup_domin.csv"), row.names = FALSE)
write.csv(tree_health_vf_lookup, paste0("outputs\\lookup_tables\\", this.sheet, "_vf_lookup.csv"), row.names = FALSE)
datatable(
  tree_health_vf_lookup %>% 
    mutate(value = round(value, 2)), # round to 2 decimal places
  rownames = FALSE,
  extensions = 'Buttons',
  options = list(
    dom = 'Btip',  
    buttons = c('copy', 'csv')
  )
)
```

DOMIN class lookup table

```{r tree_health_lookup_table_domin, warning=FALSE}
datatable(
  tree_health_vf_lookup_domin,
  rownames = FALSE,
  extensions = 'Buttons',
  options = list(
    dom = 'Btip',  
    buttons = c('copy', 'csv')
  )
)
```

## Ground flora

```{r ground_flora_function_interpolation, warning=FALSE}
this.sheet <- "Ground flora"
x.axis_title <- "% Appropraite ground flora species"
plot_data <- generate_plot_data(this.sheet, this.round, c.df, interpolation_measurements)
```

Again used a quantile GAM, because outlier value functions requiring more species were influencing a linear estimate too much. Tuned the quantile manually to hit the approximate median of values at key measurements along the function (25, 50 and 75% appropriate species). This means, at least at these points, the value function is aproximately in the middle of the estimations from the respondents.

```{r ground_flora_model, warning=FALSE, message=FALSE}
# test binomial and linear GAMs
## binomial GAM
ground_flora_gam1 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 10)  ,
  data = plot_data,
  family = binomial(link = "logit")
)
## linear GAM
ground_flora_gam2 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 10)  ,
  data = plot_data,
  family = gaussian(link = "identity")
)
## Quantile GAM
ground_flora_gam3 <- qgam(
  value.dec ~ s(measure, bs = "cs", k = 10),
  data = plot_data,
  qu = 0.51
)
# Compare AIC
aic_ground_flora <- AIC(ground_flora_gam1, ground_flora_gam2, ground_flora_gam3)
# Choose the best model based on AIC
ground_flora_gam <- ground_flora_gam3 ####!!!!!
# dummy data for prediction
newdata <- data.frame(
    measure = rep(unique(plot_data$measure), each = 1)#,
    #respondant_name = rep(unique(plot_data$respondant_name), times = length(unique(plot_data$measure)))
)
newdata <- add_gam_predictions(ground_flora_gam, newdata, scale_to_100 = TRUE)

ground_flora_predictions <- newdata %>%
  select(measure, value_plot) %>%
  rename(value = value_plot)

# original points
original_points <- c.df %>%
  filter(sheet_name == this.sheet, round == this.round)
# add prediction
original_points <- original_points %>%
  mutate(pred_value = predict(
    ground_flora_gam, 
    newdata = data.frame(measure = measure), 
    type = "response"
    ))
range_pred <- range(original_points$pred_value, na.rm = TRUE)
original_points$pred_value <- (original_points$pred_value - range_pred[1]) * 100 / (range_pred[2] - range_pred[1])
test_measures_considered <-  c(25, 50, 75)
# pull pred down? - is value bigger than prediction?
test_pred <- original_points [original_points$measure %in%test_measures_considered, ] # where prediction is less than value
# table(test_pred$pred_value > test_pred$value)

ggplot() +
  # Original data points
  # geom_point(
  #   data = original_points,
  #   aes(x = measure, y = value, color = respondant_name),
  #   alpha = 0.8, size = 0.2
  # ) +
  # Respondent interpolated lines
  geom_line(
    data = original_points,
    aes(x = measure, y = value, color = respondant_name, group = respondant_name),
    alpha = 0.2, size = 0.6
  ) +
  # Fitted overall GAM line
  geom_line(
    data = newdata,
    aes(x = measure, y = value_plot),
    color = "black", size = 1.2
  ) +
  scale_color_manual(values = respondant_colours) +
  labs(
    x = x.axis_title,
    y = "Value",
    color = "Respondent",
    title = paste(this.sheet, "estimated value function")
  ) +
  theme_pubr()+
  theme(legend.position = "none")
```

Lookup table

```{r ground_flora_lookup_table, warning=FALSE}
ground_flora_vf_lookup <- newdata %>%
  select(measure, value_plot) %>%
  mutate(measure = round(measure, 2)) %>% 
  rename(!!x.axis_title := measure,
           value = value_plot) %>%
  #remove duplicates
  distinct()
# save lookup table as csv
write.csv(ground_flora_vf_lookup, paste0("outputs\\lookup_tables\\", this.sheet, "_vf_lookup.csv"), row.names = FALSE)
datatable(
  ground_flora_vf_lookup %>% 
    mutate(value = round(value, 2)), # round to 2 decimal places
  rownames = FALSE,
  extensions = 'Buttons',
  options = list(
    dom = 'Btip',  
    buttons = c('copy', 'csv')
  )
)
```

## Horizontal complexity

```{r horizontal_complexity_function_interpolation, warning=FALSE}
this.sheet <- "Horizontal complexity"
x.axis_title <- "Horizontal Complexity Classes"
plot_data <- generate_plot_data(this.sheet, this.round, c.df, interpolation_measurements) %>% 
  #filter out all rows where measure is 0. Will add back in later
  filter(measure != 0) 
# scale to 0-1
plot_data$value.dec <- plot_data$value.dec - min(plot_data$value.dec, na.rm = TRUE)
plot_data$value.dec <- plot_data$value.dec / max(plot_data$value.dec, na.rm = TRUE)


```

Estimates trend using a linear GAM. First cut out all rows where measure is 0, so as not to affect the smooth (value was always 0 for both 0 and 1 categories), then add back in to prediction after for figure and lookup table. It just gives an option in the unlikley event that no vegetation of any height is detected.

```{r horizontal_complexity_model, warning=FALSE, message=FALSE}
# test binomial and linear GAMs
## binomial GAM
horizontal_complexity_gam1 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 5)  ,
  data = plot_data,
  family = binomial(link = "logit")
)
## linear GAM
horizontal_complexity_gam2 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 4)  ,
  data = plot_data,
  family = gaussian(link = "identity")
)
## Quantile GAM
horizontal_complexity_gam3 <- qgam(
  value.dec ~ s(measure, bs = "cs", k = 5),
  data = plot_data,
  qu = 0.5
)
# Compare AIC
aic_horizontal_complexity <- AIC(horizontal_complexity_gam1, horizontal_complexity_gam2, horizontal_complexity_gam3)
# Choose the best model based on AIC
horizontal_complexity_gam <- horizontal_complexity_gam2 ####!!!!!
# dummy data for prediction
newdata <- data.frame(
    measure = rep(unique(plot_data$measure), each = 1)#,
    #respondant_name = rep(unique(plot_data$respondant_name), times = length(unique(plot_data$measure)))
)
newdata <- add_gam_predictions(horizontal_complexity_gam, newdata, scale_to_100 = TRUE)

# add a row with 0,0
newdata <- bind_rows(
  newdata,
  data.frame(measure = 0, value_plot = 0)
)

newdata <- newdata %>%
  arrange(measure) 

# save predictions
horizontal_complexity_predictions <- newdata %>%
  select(measure, value_plot) %>%
  rename(value = value_plot)

# original points
original_points <- c.df %>%
  filter(sheet_name == this.sheet, round == this.round)
# add prediction
original_points <- original_points %>%
  mutate(pred_value = predict(
    horizontal_complexity_gam, 
    newdata = data.frame(measure = measure), 
    type = "response"
    ))
range_pred <- range(original_points$pred_value, na.rm = TRUE)
original_points$pred_value <- (original_points$pred_value - range_pred[1]) * 100 / (range_pred[2] - range_pred[1])
test_measures_considered <-  c( 2, 4)
# pull pred down? - is value bigger than prediction?
test_pred <- original_points [original_points$measure %in%test_measures_considered, ] # where prediction is less than value
# table(test_pred$pred_value > test_pred$value)

ggplot() +
  # Original data points
  # geom_point(
  #   data = original_points,
  #   aes(x = measure, y = value, color = respondant_name),
  #   alpha = 0.8, size = 0.2
  # ) +
  # Respondent interpolated lines
  geom_line(
    data = original_points,
    aes(x = measure, y = value, color = respondant_name, group = respondant_name),
    alpha = 0.2, size = 0.6
  ) +
  # Fitted overall GAM line
  geom_line(
    data = newdata,
    aes(x = measure, y = value_plot),
    color = "black", size = 1.2
  ) +
  scale_color_manual(values = respondant_colours) +
  labs(
    x = x.axis_title,
    y = "Value",
    color = "Respondent",
    title = paste(this.sheet, "estimated value function")
  ) +
  theme_pubr()+
  theme(legend.position = "none")
```

Lookup table

```{r horizontal_complexity_lookup_table, warning=FALSE}
horizontal_complexity_vf_lookup <- newdata %>%
  select(measure, value_plot) %>%
  mutate(measure = round(measure, 2)) %>% 
  rename(!!x.axis_title := measure,
           value = value_plot) %>%
  #remove duplicates
  distinct()
# save lookup table as csv
write.csv(horizontal_complexity_vf_lookup, paste0("outputs\\lookup_tables\\", this.sheet, "_vf_lookup.csv"), row.names = FALSE)
datatable(
  horizontal_complexity_vf_lookup %>% 
    mutate(value = round(value, 2)), # round to 2 decimal places
  rownames = FALSE,
  extensions = 'Buttons',
  options = list(
    dom = 'Btip',  
    buttons = c('copy', 'csv')
  )
)
```

## Anthropogenic damage

```{r anthropogenic_damage_function_interpolation, warning=FALSE}
this.sheet <- "Anthropogenic damage"
x.axis_title <- "Anthropogenic Damage cover (%)"
plot_data <- generate_plot_data(this.sheet, this.round, c.df, interpolation_measurements)

```

Here I used a quantile GAM again, its definitly the least reliable of the indicators and was difficult to tune (here trying to hit hte median over measures at 10, 20 and 30% cover). The linear GAM definitly gave too much weight to outlier vfs that suggested a very slow decline in value with increasing anthropogenic damage.

I think bootstrapping by respondent, finding mediand and fitting a spline to that would be a good way to improve this model, but I didn't have time to implement it here.

```{r anthropogenic_damage_model, warning=FALSE, message=FALSE}
# test binomial and linear GAMs
## binomial GAM
anthropogenic_damage_gam1 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 10)  ,
  data = plot_data,
  family = binomial(link = "logit")
)
## linear GAM
anthropogenic_damage_gam2 <- gam(
  value.dec ~ s(measure, bs = "cs", k = 10)  ,
  data = plot_data,
  family = gaussian(link = "identity")
)
## Quantile GAM
anthropogenic_damage_gam3 <- qgam(
  value.dec ~ s(measure, bs = "cs", k = 10),
  data = plot_data,
  qu = 0.2
)
# Compare AIC
aic_anthropogenic_damage <- AIC(anthropogenic_damage_gam1, anthropogenic_damage_gam2, anthropogenic_damage_gam3)
# Choose the best model based on AIC
anthropogenic_damage_gam <- anthropogenic_damage_gam3 ####!!!!!
# dummy data for prediction
newdata <- data.frame(
    measure = rep(unique(plot_data$measure), each = 1)#,
    #respondant_name = rep(unique(plot_data$respondant_name), times = length(unique(plot_data$measure)))
)
newdata <- add_gam_predictions(anthropogenic_damage_gam, newdata, scale_to_100 = TRUE)

anthropogenic_damage_predictions <- newdata %>%
  select(measure, value_plot) %>%
  rename(value = value_plot)

# original points
original_points <- c.df %>%
  filter(sheet_name == this.sheet, round == this.round)
# add prediction
original_points <- original_points %>%
  mutate(pred_value = predict(
    anthropogenic_damage_gam, 
    newdata = data.frame(measure = measure), 
    type = "response"
    ))
range_pred <- range(original_points$pred_value, na.rm = TRUE)
original_points$pred_value <- (original_points$pred_value - range_pred[1]) * 100 / (range_pred[2] - range_pred[1])
test_measures_considered <-  c(10, 20)
# pull pred down? - is value bigger than prediction?
test_pred <- original_points [original_points$measure %in%test_measures_considered, ] # where prediction is less than value
# table(test_pred$pred_value > test_pred$value)

ggplot() +
  # Original data points
  # geom_point(
  #   data = original_points,
  #   aes(x = measure, y = value, color = respondant_name),
  #   alpha = 0.8, size = 0.2
  # ) +
  # Respondent interpolated lines
  geom_line(
    data = original_points,
    aes(x = measure, y = value, color = respondant_name, group = respondant_name),
    alpha = 0.2, size = 0.6
  ) +
  # Fitted overall GAM line
  geom_line(
    data = newdata,
    aes(x = measure, y = value_plot),
    color = "black", size = 1.2
  ) +
  scale_color_manual(values = respondant_colours) +
  labs(
    x = x.axis_title,
    y = "Value",
    color = "Respondent",
    title = paste(this.sheet, "estimated value function")
  ) +
  theme_pubr()+
  theme(legend.position = "none")
```

Lookup table

```{r anthropogenic_damage_lookup_table, warning=FALSE}
anthropogenic_damage_vf_lookup <- newdata %>%
  select(measure, value_plot) %>%
  mutate(measure = round(measure, 2))

# make an alternative DOMIN class lookup table, taking the median value across the range of measurements
anthropogenic_damage_vf_lookup_domin <- make_domin_lookup(anthropogenic_damage_vf_lookup, summary_func = median) %>% 
  mutate(value = round(value, 2))

anthropogenic_damage_vf_lookup <- anthropogenic_damage_vf_lookup %>% 
  rename(!!x.axis_title := measure,
           value = value_plot) %>%
  #remove duplicates
  distinct()
# save lookup table as csv
write.csv(anthropogenic_damage_vf_lookup_domin, paste0("outputs\\lookup_tables\\", this.sheet, "_vf_lookup_domin.csv"), row.names = FALSE)
write.csv(anthropogenic_damage_vf_lookup, paste0("outputs\\lookup_tables\\", this.sheet, "_vf_lookup.csv"), row.names = FALSE)
datatable(
  anthropogenic_damage_vf_lookup %>% 
    mutate(value = round(value, 2)), # round to 2 decimal places
  rownames = FALSE,
  extensions = 'Buttons',
  options = list(
    dom = 'Btip',  
    buttons = c('copy', 'csv')
  )
)
```

DOMIN class lookup table

```{r anthropogenic_damage_lookup_table_domin, warning=FALSE}
datatable(
  anthropogenic_damage_vf_lookup_domin,
  rownames = FALSE,
  extensions = 'Buttons',
  options = list(
    dom = 'Btip',  
    buttons = c('copy', 'csv')
  )
)
```

## Microhabitats

The microhabitats vf was not covered in the delphi process, and instead was estimated based on the concensus expert opinion of EMcH, SH and MH.

```{r microhabitats_curation, warning=FALSE}
this.sheet <- "Microhabitats"
x.axis_title <- "Proportion of microhabitats present (%)"

# No curation just a value function based on expert opinion
microhab_vf <- data.frame(
  measure = seq(0,100, by = 10),
  value = c(0, 5, 15, 30, 50, 70,
            85, 95, 100, 100, 100) # expert opinion
)
microhab_vf <- microhab_vf %>%
  mutate(value.dec = value / 100) # scale to 0-1

anthropogenic_damage_gam <- gam(
  value.dec ~ s(measure, bs = "cs", k = 10)  ,
  data = microhab_vf,
  family = gaussian(link = "identity")
)

# dummy data for prediction
newdata <- data.frame(
    measure = seq(min(microhab_vf$measure), max(microhab_vf$measure, length = 100 ))#,
    #respondant_name = rep(unique(plot_data$respondant_name), times = length(unique(plot_data$measure)))
)
newdata <- add_gam_predictions(anthropogenic_damage_gam, newdata, scale_to_100 = TRUE)

# fix all the values after the first 100 to 100
newdata$value_plot[newdata$measure > newdata$measure[newdata$value_plot == max(newdata$value_plot)]] <- 100

# save predictions
microhab_vf_predictions <- newdata %>%
  select(measure, value_plot) %>%
  rename(value = value_plot)

# plot
ggplot() +
  # Original data points
  geom_point(
    data = microhab_vf,
    aes(x = measure, y = value),
    color = "blue", size = 2
  ) +
  # Fitted overall GAM line
  geom_line(
    data = newdata,
    aes(x = measure, y = value_plot),
    color = "black", size = 1.2
  ) +
  labs(
    x = "Microhabitats Measure (%)",
    y = "Value",
    title = paste(this.sheet, "estimated value function")
  ) +
  theme_pubr()+
  theme(legend.position = "none")


```

```{r microhabitats_lookup_table, warning=FALSE}
microhab_vf_lookup <- newdata %>%
  select(measure, value_plot) %>%
  rename('Proportion of microhabitats present'= measure,
           value = value_plot) %>%
  #remove duplicates
  distinct()
# save lookup table as csv
write.csv(microhab_vf_lookup, paste0("outputs\\lookup_tables\\", this.sheet, "_vf_lookup.csv"), row.names = FALSE)
datatable(
  microhab_vf_lookup %>% 
    mutate(value = round(value, 2)), # round to 2 decimal places
  rownames = FALSE,
  extensions = 'Buttons',
  options = list(
    dom = 'Btip',  
    buttons = c('copy', 'csv')
  )
)
```

# Estimating weights

The median weight for each indicator was estimated, and then scaled to be out of 100.

Note that the microhabitats indicator was not included in the delphi process. Its weight was set to 100, based on expert concensus of EMcH, MH and SH, as it is considered to be of equal importance to the most important indicator.


```{r weights_estimation, warning=FALSE, message=FALSE}
# distill down to just one row from each indicator-respondent combination from this.round

weights.df <- c.df %>%
  filter(round == this.round) %>%
  # filter just first for each indicator-respondent combination
  group_by(sheet_name, respondant_name) %>%
  slice(1) %>%
  ungroup() %>% 
  arrange(sheet_name, respondant_name) 

# calcualte median for each indicator and scale to max 100
weights.df_ind <- weights.df %>%
  group_by(sheet_name) %>%
  summarise(
    median_weight = median(weight, na.rm = TRUE),
    indicator_name = first(sheet_name),
  ) %>%
  ungroup() %>%
  mutate(median_weight_scaled = median_weight / max(median_weight, na.rm = TRUE) * 100) %>% 
  # remove woodland extent and add microhabitats, with a median_weight_scaled of 100
  bind_rows(
    data.frame(
      indicator_name = "Microhabitats",
      median_weight = 100,
      median_weight_scaled = 100
    )
  )
```
```{r weights_plot, warning=FALSE, message=FALSE, caption="Estimated importance weights for each indicator of woodland  condition (Black). Background coloured points show the individual respondants estiamtes, and the grey circles indicate their median. These medain scores were scaled to max 100 for the final estiamtes (black circles)"}

# plot weights
## in background weights.df weights, coloured by respondant, then median, with transparency, then rescaled version over top in black

ggplot()+
  geom_point(
    data = weights.df,
    aes(x = sheet_name, y = weight, color = respondant_name),
    alpha = 0.2, size = 1
  ) +
  geom_point(
    data = weights.df_ind,
    aes(x = indicator_name, y = median_weight),
    color = "black", alpha = 0.2, size = 3
  ) +
  geom_point(
    data = weights.df_ind,
    aes(x = indicator_name, y = median_weight_scaled),
    color = "black", size = 5
  ) +
  coord_flip() +
  labs(
    x = "Indicator",
    y = "Weight",
    title = paste("Importance Weights for indicators from Delphi round", this.round)
  ) +
  scale_color_manual(values = respondant_colours) +
  theme_pubr() +
  theme(legend.position = "none")

```

Lookup table
```{r weights_lookup_table, warning=FALSE}
weights_lookup <- weights.df_ind %>%
  select(indicator_name, median_weight_scaled) %>%
  rename('Indicator' = indicator_name,
         'Weight (scaled to max 100)' = median_weight_scaled) %>%
  #remove duplicates
  distinct()
# save lookup table as csv
write.csv(weights_lookup, paste0("outputs\\lookup_tables\\weights_lookup.csv"), row.names = FALSE)
datatable(
  weights_lookup %>% 
    mutate(`Weight (scaled to max 100)` = round(`Weight (scaled to max 100)`, 2)), # round to 2 decimal places
  rownames = FALSE,
  extensions = 'Buttons',
  options = list(
    dom = 'Btip',  
    buttons = c('copy', 'csv')
  )
)
```
```{r save_all_value_functions, warning=FALSE}
# save all value functions at once
save(
  age_structure_predictions,
  native_canopy_predictions,
  vertical_structure_predictions,
  n_tree_shrub_spp_predictions,
  invasive_plants_predictions,
  deadwood_predictions,
  veteran_trees_predictions,
  regen_predictions,
  herbivore_damage_predictions,
  tree_health_predictions,
  ground_flora_predictions,
  horizontal_complexity_predictions,
  anthropogenic_damage_predictions,
  microhab_vf_predictions,
  
  file = "outputs\\all_value_functions.RData"
)
```